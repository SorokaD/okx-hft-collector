version: '3.8'

services:

  minio:
    image: minio/minio:latest
    container_name: minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minioadmin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minioadmin123}
      MINIO_REGION_NAME: ${MINIO_REGION_NAME:-us-east-1}
    volumes:
      - minio_data:/data
    ports:
      - "9002:9000"   # S3 API
      - "9003:9001"   # MinIO Console
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/ready"]
      interval: 15s
      timeout: 5s
      retries: 10
    restart: unless-stopped

  minio-init:
    image: minio/mc:latest
    container_name: minio-init
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      mc alias set local http://minio:9000 $${MINIO_ROOT_USER} $${MINIO_ROOT_PASSWORD} &&
      mc mb --ignore-existing local/models &&
      mc anonymous set none local/models &&
      echo 'Bucket models is ready'
      "
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minioadmin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minioadmin123}
    restart: "no"

  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data

  clickhouse:
    image: clickhouse/clickhouse-server:latest
    container_name: clickhouse
    ports:
      - "8123:8123"
      - "9000:9000"
      - "9009:9009" # interserver port
    volumes:
      - /mnt/d/clickhouse:/var/lib/clickhouse
      - ./config/users.xml:/etc/clickhouse-server/users.xml:ro
      - ./clickhouse_logs:/var/log/clickhouse-server
    ulimits:
      nofile:
        soft: 262144
        hard: 262144

  tabix:
    image: spoonest/clickhouse-tabix-web-client
    container_name: tabix
    ports:
      - "8080:80"
    depends_on:
      - clickhouse

  mlflow:
    image: ghcr.io/mlflow/mlflow
    container_name: mlflow
    ports:
      - "5000:5000"
    volumes:
      - ./mlruns:/mlflow/mlruns
    command: mlflow server --host 0.0.0.0 --backend-store-uri sqlite:///mlflow.db --default-artifact-root /mlflow/mlruns

  airflow:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    #image: apache/airflow:2.8.1-python3.10
    container_name: airflow
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
    depends_on:
      - postgres
    ports:
      - "8081:8080"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    # command: >
      # bash -c "
      #   airflow db upgrade &&
      #   airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com &&
      #   exec airflow webserver
      # "
      
    command: >
      bash -c "
        until pg_isready -h postgres -p 5432; do
          echo 'â³ Waiting for PostgreSQL...';
          sleep 3;
        done &&
        airflow db upgrade &&
        airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com &&
        exec airflow webserver"

  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    #image: apache/airflow:2.8.1-python3.10
    container_name: airflow-scheduler
    depends_on:
      - airflow
      - postgres
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    # command: >
    #   bash -c "
    #     airflow db upgrade &&
    #     exec airflow scheduler
    #   "
    command: >
      bash -c "
        until pg_isready -h postgres -p 5432; do
          echo 'â³ Waiting for PostgreSQL...';
          sleep 3;
        done &&
        airflow db upgrade &&
        exec airflow scheduler"


  superset:
    image: apache/superset
    container_name: superset
    environment:
      - SUPERSET_LOAD_EXAMPLES=no
      - SUPERSET_CONFIG_PATH=/app/superset_home/superset_config.py
    ports:
      - "8088:8088"
    volumes:
      - ./superset_home:/app/superset_home
    command: >
      bash -c "
        superset db upgrade &&
        superset fab create-admin --username admin --firstname Superset --lastname Admin --email admin@superset.com --password admin &&
        superset init &&
        superset run -h 0.0.0.0 -p 8088
      "

volumes:
  clickhouse_data:
  postgres_data:
  minio_data:
